---
title: "Practical Statistics Exercises"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

##### KEY TERMS FOR DATA TYPES
Continuous  
  
* Data that can take on any value in an interval. (Synonyms: interval, float, numeric)
    
Discrete  

* Data that can take on only integer values, such as counts. (Synonyms: integer, count)
    
Categorical  

* Data that can take on only a specific set of values representing a set of possible categories. (Synonyms: enums, enumerated, factors, nominal, polychotomous)
    
Binary  

* A special case of categorical data with just two categories of values (0/1, true/false). (Synonyms: dichotomous, logical, indicator, boolean)
    
Ordinal  
  
* Categorical data that has an explicit ordering. (Synonyms: ordered factor)

There are two basic types of structured data: numeric and categorical. 

Numeric data comes in two forms:  
Continuous, such as wind speed or time duration.  
Discrete, such as the count of the occurrence of an event. 

Categorical data takes only a fixed set of values, such as a type of TV screen (plasma, LCD, LED,
etc.) or a state name (Alabama, Alaska, etc.). 
Binary data is an important special case of categorical data
that takes on only one of two values, such as 0/1, yes/no, or true/false. 
Another useful type of categorical data is ordinal data in which the categories are ordered; an example of this is a numerical rating (1, 2, 3,
4, or 5).


##### KEY TERMS FOR RECTANGULAR DATA

Data frame  

* Rectangular data (like a spreadsheet) is the basic data structure for statistical and machine learning models.

Feature

* A column in the table is commonly referred to as a feature. (Synonyms: attribute, input, predictor, variable)

Outcome

* Many data science projects involve predicting an outcome — often a yes/no outcome. 
* The features are sometimes used to predict the outcome in an experiment or study. (Synonyms: dependent variable, response, target, output)

Records

* A row in the table is commonly referred to as a record. (Synonyms: case, example, instance, observation, pattern, sample)
    
    
    
#### Estimates of Location
Variables with measured or count data might have thousands of distinct values. A basic step in exploring
your data is getting a “typical value” for each feature (variable): an estimate of where most of the data is
located (i.e., its central tendency).

##### KEY TERMS FOR ESTIMATES OF LOCATION
Mean  

* The sum of all values divided by the number of values.(Synonyms:average)

$$Mean = \overline{x} = \frac{\sum_{i=1}^n x_i}{n}$$

Weighted mean  

* The sum of all values times a weight divided by the sum of the weights. (Synonyms: weighted average)

Median  

* The value such that one-half of the data lies above and below. (Synonyms: 50th percentile)

Weighted median  

* The value such that one-half of the sum of the weights lies above and below the sorted data.

$$Weighted\,mean = \overline{x}_w = \frac{\sum_{i=1}^{n} w_i x_i}{\sum_{i}^{n}w_i}$$

Trimmed mean  

* The average of all values after dropping a fixed number of extreme values. (Synonyms: truncated mean)
$$Trimmed\,mean = \overline{x} = \frac{\sum_{i=p+1}^{n-p} x_i}{n-2p}$$

Robust  

* Not sensitive to extreme values. (Synonyms: resistant)

Outlier  

* A data value that is very different from most of the data. (Synonyms: extreme value)
    
```{r mean}

state <- read.csv(file="C:/Users/Emilio Blanco/Documents/Emilio/DS/datascience-scripts/psds_data/state.csv")
mean(state[["Population"]])

mean(state[["Population"]], trim=0.1)

median(state[["Population"]])

```

The mean is bigger than the trimmed mean, which is bigger than the median.
This is because the trimmed mean excludes the largest and smallest five states (trim=0.1 drops 10%
from each end). If we want to compute the average murder rate for the country, we need to use a weighted
mean or median to account for different populations in the states. Since base R doesn’t have a function for
weighted median, we need to install a package such as matrixStats:

```{r weighted mean}
weighted.mean(state[["Murder.Rate"]], w=state[["Population"]])

library("matrixStats")
weightedMedian(state[["Murder.Rate"]], w=state[["Population"]])

```

#### Estimates of Variability
Location is just one dimension in summarizing a feature. A second dimension, __variability__, also referred to
as __dispersion__, measures whether the data values are tightly clustered or spread out. At the heart of
statistics lies variability: measuring it, reducing it, distinguishing random from real variability, identifying
the various sources of real variability, and making decisions in the presence of it.

##### KEY TERMS FOR VARIABILITY METRICS
Deviations  

* The difference between the observed values and the estimate of location. (Synonyms: errors, residuals)

Variance  

* The sum of squared deviations from the mean divided by n – 1 where n is the number of data values. (Synonyms: mean-squared-error)

$$Variance =s^2= \frac{\sum (x-\overline{x})^2}{n-1}$$

Standard deviation  

* The square root of the variance. (Synonyms: l2-norm, Euclidean norm)

$$Standard \,deviation =s= \sqrt{Variance}$$

Mean absolute deviation  

* The mean of the absolute value of the deviations from the mean. (Synonyms: l1-norm, Manhattan norm)

$$Mean\,Absolute\,Deviation = \frac{\sum_{i=1}^n |x_i-\overline{x}|}{n}$$

Median absolute deviation from the median  

* The median of the absolute value of the deviations from the median.
  
$$Median\,Absolute\,Deviation = Median (|x_1-m|,|x_2-m|, ..., |x_N-m|)$$  
where m is the median. Like the median, the MAD is not influenced by extreme values. It is also possible
to compute a trimmed standard deviation analogous to the trimmed mean

Range  

* The difference between the largest and the smallest value in a data set.

Order statistics  

* Metrics based on the data values sorted from smallest to biggest. (Synonyms: ranks)

Percentile  

* The value such that P percent of the values take on this value or less and (100–P) percent take on this value or more. (Synonyms: quantile)  

If we have an even number of data (n is even), then the percentile is ambiguous under the preceding definition. In fact, we could
take on any value between the order statistics $x_{(j)}$ and $x_{(j+1)}$where j satisfies:    
    $$100 * \frac {j}{n} \leq P < 100 * \frac {j+1}{n}  $$  
  Formally, the percentile is the weighted average:
  $$Percentile (P) = (1-w)x_{(j)} + wx_{(j+1)}  $$ 

  for some weight w between 0 and 1. Statistical software has slightly differing approaches to choosing w. In fact, the R function
  quantile offers nine different alternatives to compute the quantile. Except for small data sets, you don’t usually need to worry
  about the precise way a percentile is calculated.

Interquartile range
  The difference between the 75th percentile and the 25th percentile.
  Synonyms
    IQR


#### Estimates Based on Percentiles
A different approach to estimating dispersion is based on looking at the spread of the sorted data.
Statistics based on sorted (ranked) data are referred to as order statistics. The most basic measure is the
range: the difference between the largest and smallest number. The minimum and maximum values
themselves are useful to know, and helpful in identifying outliers, but the range is extremely sensitive to
outliers and not very useful as a general measure of dispersion in the data.

To avoid the sensitivity to outliers, we can look at the range of the data after dropping values from each
end. Formally, these types of estimates are based on differences between percentiles. In a data set, the Pth
percentile is a value such that at least P percent of the values take on this value or less and at least (100 –
P) percent of the values take on this value or more. For example, to find the 80th percentile, sort the data.
Then, starting with the smallest value, proceed 80 percent of the way to the largest value. Note that the
median is the same thing as the 50th percentile. The percentile is essentially the same as a quantile, with
quantiles indexed by fractions (so the .8 quantile is the same as the 80th percentile).

A common measurement of variability is the difference between the 25th percentile and the 75th
percentile, called the interquartile range (or IQR). 
Here is a simple example: 3,1,5,3,6,7,2,9. We sort these to get 1,2,3,3,5,6,7,9. 
The 25th percentile is at 2.5, and the 75th percentile is at 6.5, so the
interquartile range is 6.5 – 2.5 = 4. 
Software can have slightly differing approaches that yield different
answers (see the following note); typically, these differences are smaller.
For very large data sets, calculating exact percentiles can be computationally very expensive since it
requires sorting all the data values. Machine learning and statistical software use special algorithms, such
as [Zhang-Wang-2007], to get an approximate percentile that can be calculated very quickly and is
guaranteed to have a certain accuracy.

Using R’s built-in functions for the standard deviation, interquartile range (IQR), and the median
absolution deviation from the median (MAD), we can compute estimates of variability for the state
population data:
```{r variability}

sd(state[["Population"]])

IQR(state[["Population"]])

mad(state[["Population"]])

```
The standard deviation is almost twice as large as the MAD (in R, by default, the scale of the MAD is
adjusted to be on the same scale as the mean). This is not surprising since the standard deviation is
sensitive to outliers.

##### KEY IDEAS
* The variance and standard deviation are the most widespread and routinely reported statistics of variability.
* Both are sensitive to outliers.
* More robust metrics include mean and median absolute deviations from the mean and percentiles (quantiles).

#### Exploring the Data Distribution

Each of the estimates we’ve covered sums up the data in a single number to describe the location or variability of the data. It is also useful to explore how the data is distributed overall.

##### KEY TERMS FOR EXPLORING THE DISTRIBUTION

Boxplot  

* A plot introduced by Tukey as a quick way to visualize the distribution of data. (Synonyms:Box and whiskers plot)

Frequency table  

* A tally of the count of numeric data values that fall into a set of intervals (bins).

Histogram  

* A plot of the frequency table with the bins on the x-axis and the count (or proportion) on the y-axis.

Density plot  

* A smoothed version of the histogram, often based on a kernal density estimate.

##### Percentiles and Boxplots

we explored how percentiles can be used to measure the spread of
the data. Percentiles are also valuable to summarize the entire distribution. It is common to report the
quartiles (25th, 50th, and 75th percentiles) and the deciles (the 10th, 20th, …, 90th percentiles).
Percentiles are especially valuable to summarize the tails (the outer range) of the distribution. Popular
culture has coined the term one-percenters to refer to the people in the top 99th percentile of wealth.
```{r quantile}

quantile(state[["Murder.Rate"]], p=c(.05, .25, .5, .75, .95))

```

The median is 4 murders per 100,000 people, although there is quite a bit of variability: the 5th percentile
is only 1.6 and the 95th percentile is 6.51.

Boxplots, introduced by Tukey [Tukey-1977], are based on percentiles and give a quick way to visualize
the distribution of data. Figure 1-2 shows a boxplot of the population by state produced by R:
```{r boxplot}

boxplot(state[["Population"]]/1000000, ylab="Population (millions)")

```  

* The top and bottom of the box are the 75th and 25th percentiles, respectively.  
* The median is shown by the horizontal line in the box.  
* The dashed lines, referred to as whiskers, extend from the top and bottom to indicate the range for the bulk of the data.  

There are many variations of a boxplot; see, for example, the
documentation for the R function boxplot [R-base-2015]. By default, the R function extends the whiskers
to the furthest point beyond the box, except that it will not go beyond 1.5 times the IQR (other software
may use a different rule). Any data outside of the whiskers is plotted as single points.

##### Frequency Table and Histograms  
A **frequency table** of a variable divides up the variable range into equally spaced segments, and tells us
how many values fall in each segment.  
Frequency table of the population by state computed in R:
```{r frequency}

breaks <- seq(from=min(state[["Population"]]), to=max(state[["Population"]]), length=11)
pop_freq <- cut(state[["Population"]], breaks=breaks, right=TRUE, include.lowest = TRUE)
table(pop_freq)

```  

A **histogram** is a way to visualize a frequency table, with bins on the x-axis and data count on the y-axis.
To create a histogram corresponding to Table 1-5 in R, use the hist function with the breaks argument:  

```{r histogram}
hist(state[["Population"]], breaks=breaks)
```  

In general, histograms are plotted such that:  

* Empty bins are included in the graph.  
* Bins are equal width.  
* Number of bins (or, equivalently, bin size) is up to the user.  
* Bars are contiguous — no empty space shows between bars, unless there is an empty bin.


> In statistical theory, location and variability are referred to as the first and second moments of a distribution. The third and fourth
moments are called skewness and kurtosis. Skewness refers to whether the data is skewed to larger or smaller values and
kurtosis indicates the propensity of the data to have extreme values. Generally, metrics are not used to measure skewness and
kurtosis; instead, these are discovered through visual displays

##### Density Estimates

Related to the histogram is a **density** **plot**, which shows the distribution of data values as a continuous
line. A density plot can be thought of as a smoothed histogram, although it is typically computed directly
from the data through a kernal density estimate (see [Duong-2001] for a short tutorial).  
Figure 1-4 displays a density estimate superposed on a histogram. In R, you can compute a density estimate using the
density function:
```{r density}
hist(state[["Murder.Rate"]], freq=FALSE)
lines(density(state[["Murder.Rate"]]), lwd=3, col="blue")
```  


A key distinction from the histogram plotted in Figure 1-3 is the scale of the y-axis: a density plot
corresponds to plotting the histogram as a proportion rather than counts (you specify this in R using the argument freq=FALSE).

##### KEY IDEAS  

* A frequency histogram plots frequency counts on the y-axis and variable values on the x-axis; it gives a sense of the distribution of
the data at a glance.  
* A frequency table is a tabular version of the frequency counts found in a histogram.  
* A boxplot — with the top and bottom of the box at the 75th and 25th percentiles, respectively — also gives a quick sense of the
distribution of the data; it is often used in side-by-side displays to compare distributions.  
* A density plot is a smoothed version of a histogram; it requires a function to estimate a plot based on the data (multiple estimates
are possible, of course).

#### Exploring Binary and Categorical Data  
For **categorical** **data**, simple proportions or percentages tell the story of the data.  

##### KEY TERMS FOR EXPLORING CATEGORICAL DATA  

Mode

* The most commonly occurring category or value in a data set.  

Expected value  

* When the categories can be associated with a numeric value, this gives an average value based on a category’s probability of occurrence.

Bar charts  

* The frequency or proportion for each category plotted as bars.  

Pie charts  

* The frequency or proportion for each category plotted as wedges in a pie.

Getting a summary of a binary variable or a categorical variable with a few categories is a fairly easy matter: we just figure out the proportion of 1s, or of the important categories.

Bar charts are a common visual tool for displaying a single categorical variable, often seen in the popular
press. Categories are listed on the x-axis, and frequencies or proportions on the y-axis. Figure 1-5 shows
the airport delays per year by cause for Dallas/Fort Worth, and it is produced with the R function
barplot:

```{r barplot}
dfw <- read.csv(file="C:/Users/Emilio Blanco/Documents/Emilio/DS/datascience-scripts/psds_data/dfw_airline.csv")
barplot(as.matrix(dfw)/6, cex.axis=.5)
```


##### Expected Value
A special type of categorical data is data in which the categories represent or can be mapped to discrete
values on the same scale. A marketer for a new cloud technology, for example, offers two levels of
service, one priced at \$300/month and another at $50/month. The marketer offers free webinars to
generate leads, and the firm figures that 5% of the attendees will sign up for the $300 service, 15% for the
$50 service, and 80% will not sign up for anything. This data can be summed up, for financial purposes,
in a single **“expected value,” which is a form of weighted mean in which the weights are probabilities.**  

The expected value is calculated as follows:
$$EV = (0.05)(300) + (0.15)(50) + (0.80)(0) = 22.5 $$

1. Multiply each outcome by its probability of occurring.
2. Sum these values.  

In the cloud service example, the expected value of a webinar attendee is thus $22.50 per month,
calculated as follows:
The expected value is really a form of weighted mean: it adds the ideas of future expectations and
probability weights, often based on subjective judgment. Expected value is a fundamental concept in
business valuation and capital budgeting — for example, the expected value of five years of profits from a
new acquisition, or the expected cost savings from new patient management software at a clinic.

##### KEY IDEAS  

* Categorical data is typically summed up in proportions, and can be visualized in a bar chart.  
* Categories might represent distinct things (apples and oranges, male and female), levels of a factor variable (low, medium, and
high), or numeric data that has been binned.  
* Expected value is the sum of values times their probability of occurrence, often used to sum up factor variable levels.

#### Correlation  

Exploratory data analysis in many modeling projects (whether in data science or in research) involves
examining correlation among predictors, and between predictors and a target variable. Variables X and Y
(each with measured data) are said to be positively correlated if high values of X go with high values of
Y, and low values of X go with low values of Y. If high values of X go with low values of Y, and vice
versa, the variables are negatively correlated.

##### KEY TERMS FOR CORRELATION  

Correlation coefficient  

* A metric that measures the extent to which numeric variables are associated with one another (ranges from –1 to +1).

Correlation matrix  

* A table where the variables are shown on both rows and columns, and the cell values are the correlations between the variables.

Scatterplot  

* A plot in which the x-axis is the value of one variable, and the y-axis the value of another.

Consider these two variables, perfectly correlated in the sense that each goes from low to high:  

* v1: {1, 2, 3}  

* v2: {4, 5, 6}

The vector sum of products is 4 + 10 + 18 = 32.Now try shuffling one of them and recalculating — the
vector sum of products will never be higher than 32. So this sum of products could be used as a metric;
that is, the observed sum of 32 could be compared to lots of random shufflings (in fact, this idea relates to
a resampling-based estimate: see “Permutation Test”). Values produced by this metric, though, are not that
meaningful, except by reference to the resampling distribution.

More useful is a standardized variant: the **correlation coefficient**, which gives an estimate of the
correlation between two variables that always lies on the same scale. To compute Pearson’s correlation
coefficient, we multiply deviations from the mean for variable 1 times those for variable 2, and divide by
the product of the standard deviations:
$$r = \frac{\sum_{i=1}^N (x_i-\overline{x})(y_i-\overline{y})}{(N-1)s_xs_y}$$  
The correlation coefficient always lies between +1 (perfect positive correlation) and –1 (perfect negative
correlation); 0 indicates no correlation.  

Variables can have an association that is not linear, in which case the correlation coefficient may not be a
useful metric. The relationship between tax rates and revenue raised is an example: as tax rates increase
from 0, the revenue raised also increases. However, once tax rates reach a high level and approach
100%, tax avoidance increases and tax revenue actually declines.  

called a correlation matrix, shows the correlation between the daily returns for
telecommunication stocks from July 2012 through June 2015. From the table, you can see that Verizon
(VZ) and ATT (T) have the highest correlation. Level Three (LVLT), which is an infrastructure company,
has the lowest correlation. Note the diagonal of 1s (the correlation of a stock with itself is 1), and the
redundancy of the information above and below the diagonal.

Table 1-7. Correlation between telecommunication stock returns


|   | T | CTL | FTR | VZ | LVLT |
|:-:|:-:|:---:|:---:|:--:|:----:|
| T | 1.000 | 0.475 | 0.328 | 0.678 | 0.279 |
| CTL | 0.475 | 1.000 | 0.420 | 0.417 | 0.287 |
| FTR | 0.328 | 0.420 | 1.000 | 0.287 | 0.260 |
| VZ | 0.678 | 0.417 | 0.287 | 1.000 | 0.242 |
| LVLT | 0.279 | 0.287 | 0.260 | 0.242 | 1.000 |  


A table of correlations like Table 1-7 is commonly plotted to visually display the relationship between
multiple variables. Figure 1-6 shows the correlation between the daily returns for major exchange traded
funds (ETFs). In R, we can easily create this using the package corrplot:  
```{r corrplot}
sp500_px <- read.csv(file="C:/Users/Emilio Blanco/Documents/Emilio/DS/datascience-scripts/psds_data/sp500_data.csv")
sp500_sym <- read.csv(file="C:/Users/Emilio Blanco/Documents/Emilio/DS/datascience-scripts/psds_data/sp500_sectors.csv", stringsAsFactors = FALSE)
etfs <- sp500_px[row.names(sp500_px)>"2012-07-01", sp500_sym[sp500_sym$sector=="etf", 'symbol']]
library(corrplot)
corrplot(cor(etfs), method = "ellipse")
```  

#### Scatterplots  

The standard way to visualize the relationship between two measured data variables is with a **scatterplot.**
The x-axis represents one variable, the y-axis another, and each point on the graph is a record. See Figure
1-7 for a plot between the daily returns for ATT and Verizon. This is produced in R with the command:
```{r Scatterplots}

telecom <- sp500_px[, sp500_sym[sp500_sym$sector=="telecommunications_services", 'symbol']]
telecom <- telecom[row.names(telecom)>"2012-07-01", ]
telecom_cor <- cor(telecom)

plot(telecom$T, telecom$VZ, xlab="T", ylab="VZ")
```  


The returns have a strong positive relationship: on most days, both stocks go up or go down in tandem.
There are very few days where one stock goes down significantly while the other stock goes up (and vice
versa).


##### KEY IDEAS FOR CORRELATION  

* The correlation coefficient measures the extent to which two variables are associated with one another.  
* When high values of v1 go with high values of v2, v1 and v2 are positively associated.  
* When high values of v1 are associated with low values of v2, v1 and v2 are negatively associated.  
* The correlation coefficient is a standardized metric so that it always ranges from –1 (perfect negative correlation) to +1 (perfect positive correlation).  
* A correlation coefficient of 0 indicates no correlation, but be aware that random arrangements of data will produce both positive
and negative values for the correlation coefficient just by chance.


#### Exploring Two or More Variables  

Familiar estimators like mean and variance look at variables one at a time (univariate analysis).
Correlation analysis (see “Correlation”) is an important method that compares two variables (bivariate
analysis). In this section we look at additional estimates and plots, and at more than two variables
(multivariate analysis).  

##### KEY TERMS FOR EXPLORING TWO OR MORE VARIABLES  

Contingency tables
A tally of counts between two or more categorical variables.
Hexagonal binning
A plot of two numeric variables with the records binned into hexagons.
Contour plots
A plot showing the density of two numeric variables like a topographical map.
Violin plots
Similar to a boxplot but showing the density estimate.
Like univariate analysis, bivariate analysis involves both computing summary statistics and producing
visual displays. The appropriate type of bivariate or multivariate analysis depends on the nature of the
data: numeric versus categorical.

#### Hexagonal binding and 
```{r hex bind}
library(ggplot2)
library(hexbin)
kc_tax <- read.csv(file="C:/Users/Emilio Blanco/Documents/Emilio/DS/datascience-scripts/psds_data/kc_tax.csv")
kc_tax0 <- subset(kc_tax, TaxAssessedValue < 750000 & SqFtTotLiving>100 & SqFtTotLiving<3500)

ggplot(kc_tax0, (aes(x=SqFtTotLiving, y=TaxAssessedValue))) +
stat_binhex(colour="white") +
theme_bw() +
scale_fill_gradient(low="white", high="black") +
labs(x="Finished Square Feet", y="Tax Assessed Value")

``` 

#### Two Categorical Variables
A useful way to summarize two categorical variables is a contingency table — a table of counts by
category.

```{r cat_var}
library(descr)
lc_loans <- read.csv(file="C:/Users/Emilio Blanco/Documents/Emilio/DS/datascience-scripts/psds_data/lc_loans.csv")

x_tab <- CrossTable(lc_loans$grade, lc_loans$status, prop.c=FALSE, prop.chisq=FALSE, prop.t=FALSE)
x_tab
``` 

#### Categorical and Numeric Data

Boxplots (see “Percentiles and Boxplots”) are a simple way to visually compare the distributions of a
numeric variable grouped according to a categorical variable.

```{r delays}

airline_stats <- read.csv(file="C:/Users/Emilio Blanco/Documents/Emilio/DS/datascience-scripts/psds_data/airline_stats.csv")
boxplot(pct_carrier_delay ~ airline, data=airline_stats, ylim=c(0, 50))

```

A violin plot, introduced by [Hintze-Nelson-1998], is an enhancement to the boxplot and plots the density
estimate with the density on the y-axis. The density is mirrored and flipped over and the resulting shape is
filled in, creating an image resembling a violin. The advantage of a violin plot is that it can show nuances
in the distribution that aren’t perceptible in a boxplot. On the other hand, the boxplot more clearly shows
the outliers in the data. In ggplot2, the function geom_violin can be used to create a violin plot as
follows:

```{r violin}

ggplot(data=airline_stats, aes(airline, pct_carrier_delay)) +
ylim(0, 50) +
geom_violin() +
labs(x="", y="Daily % of Delayed Flights")

```

#### Visualizing Multiple Variables  
The types of charts used to compare two variables — scatterplots, hexagonal binning, and boxplots — are
readily extended to more variables through the notion of conditioning. As an example, look back at
Figure 1-8, which showed the relationship between homes’ finished square feet and tax-assessed values.
We observed that there appears to be a cluster of homes that have higher tax-assessed value per square
foot. Diving deeper, Figure 1-12 accounts for the effect of location by plotting the data for a set of zip
codes. Now the picture is much clearer: tax-assessed value is much higher in some zip codes (98112,
98105) than in others (98108, 98057).

```{r multiple vars}

ggplot(subset(kc_tax0, ZipCode %in% c(98188, 98105, 98108, 98126)),
aes(x=SqFtTotLiving, y=TaxAssessedValue)) +
stat_binhex(colour="white") +
theme_bw() +
scale_fill_gradient( low="white", high="blue") +
labs(x="Finished Square Feet", y="Tax Assessed Value") +
facet_wrap("ZipCode")
```
#### Chapter 2 Data and Sampling DIstributions
A popular misconception holds that the era of big data means the end of a need for sampling. In fact, the
proliferation of data of varying quality and relevance reinforces the need for sampling as a tool to work
efficiently with a variety of data and to minimize bias. Even in a big data project, predictive models are
typically developed and piloted with samples. Samples are also used in tests of various sorts (e.g.,
pricing, web treatments).
Figure 2-1 shows a schematic that underpins the concepts in this chapter. The lefthand side represents a
population that, in statistics, is assumed to follow an underlying but unknown distribution. The only thing
available is the sample data and its empirical distribution, shown on the righthand side. To get from the
lefthand side to the righthand side, a sampling procedure is used (represented by dashed arrows).
Traditional statistics focused very much on the lefthand side, using theory based on strong assumptions
about the population. Modern statistics has moved to the righthand side, where such assumptions are not
needed.

#### Random Sampling and Sample Bias

A sample is a subset of data from a larger data set; statisticians call this larger data set the population. A
population in statistics is not the same thing as in biology — it is a large, defined but sometimes
theoretical or imaginary, set of data.

##### KEY TERMS FOR RANDOM SAMPLING

Sample  

* A subset from a larger data set.

Population  

* The larger data set or idea of a data set.

N (n)  

* The size of the population (sample).

Random sampling

* Drawing elements into a sample at random.

Stratified sampling

* Dividing the population into strata and randomly sampling from each strata.

Simple random sample

* The sample that results from random sampling without stratifying the population.

Sample bias

* A sample that misrepresents the population.

Random sampling is a process in which each available member of the population being sampled has an
equal chance of being chosen for the sample at each draw. The sample that results is called a simple
random sample. Sampling can be done with replacement, in which observations are put back in the
population after each draw for possible future reselection. Or it can be done without replacement, in
which case observations, once selected, are unavailable for future draws.

Data quality in data science involves completeness, consistency of format, cleanliness, and accuracy of
individual data points. Statistics adds the notion of representativeness.

The Literary Digest opted for quantity, paying little attention to the method of selection. They ended up
polling those with relatively high socioeconomic status (their own subscribers, plus those who, by virtue
of owning luxuries like telephones and automobiles, appeared in marketers’ lists). The result was sample
bias; that is, the sample was different in some meaningful nonrandom way from the larger population it
was meant to represent.

The term nonrandom is important — hardly any sample, including random
samples, will be exactly representative of the population. Sample bias occurs when the difference is
meaningful, and can be expected to continue for other samples drawn in the same way as the first.


Bias

Statistical bias refers to measurement or sampling errors that are systematic and produced by the
measurement or sampling process. An important distinction should be made between errors due to random
chance, and errors due to bias.
Bias comes in different forms, and may be observable or invisible. When a result does suggest bias (e.g.,
by reference to a benchmark or actual values), it is often an indicator that a statistical or machine learning
model has been misspecified, or an important variable left out.

Random sampling is not always easy. Proper definition of an accessible population is key.

In stratified sampling, the population is divided up into strata, and random samples are taken from each
stratum.

#### Size versus Quality: When Does Size Matter?

In the era of big data, it is sometimes surprising that smaller is better. Time and effort spent on random
sampling not only reduce bias, but also allow greater attention to data exploration and data quality. For
example, missing data and outliers may contain useful information. It might be prohibitively expensive to
track down missing values or evaluate outliers in millions of records, but doing so in a sample of several
thousand records may be feasible.

So when are massive amounts of data needed?
The classic scenario for the value of big data is when the data is not only big, but sparse as well.


#### Sample Mean versus Population Mean
The symbol
$$\overline{x}$$
is used to represent the mean of a sample from a population, whereas
$$\mu$$  
is used to represent the mean of a population.

#### KEY IDEAS
* Even in the era of big data, random sampling remains an important arrow in the data scientist’s quiver.
* Bias occurs when measurements or observations are systematically in error because they are not representative of the full population.
* Data quality is often more important than data quantity, and random sampling can reduce bias and facilitate quality improvement that would be prohibitively expensive.

#### Selection BIAS
Selection bias refers to the practice of selectively choosing data — consciously or unconsciously — in a
way that that leads to a conclusion that is misleading or ephemeral.

#### KEY TERMS

Bias
 * Systematic error.

Data snooping
 * Extensive hunting through data in search of something interesting.

Vast search effect
 * Bias or nonreproducibility resulting from repeated data modeling, or modeling data with large numbers of predictor variables.
 
 We can guard against this by using a holdout set, and sometimes more than one holdout set, against which
to validate performance. Elder also advocates the use of what he calls target shuffling (a permutation
test, in essence) to test the validity of predictive associations that a data mining model suggests.

#### Regression to the Mean
Regression to the mean refers to a phenomenon involving successive measurements on a given variable:
extreme observations tend to be followed by more central ones. Attaching special focus and meaning to
the extreme value can lead to a form of selection bias.
Regression to the mean is a consequence of a particular form of selection bias. When we select the rookie
with the best performance, skill and good luck are probably contributing. In his next season, the skill will
still be there but, in most cases, the luck will not, so his performance will decline — it will regress.

####KEY IDEAS
* Specifying a hypothesis, then collecting data following randomization and random sampling principles, ensures against bias.
* All other forms of data analysis run the risk of bias resulting from the data collection/analysis process (repeated running of models
in data mining, data snooping in research, and after-the-fact selection of interesting events).

##### Sampling Distribution of a Statistic

The term sampling distribution of a statistic refers to the distribution of some sample statistic, over many
samples drawn from the same population. Much of classical statistics is concerned with making
inferences from (small) samples to (very large) populations.

#### KEY TERMS

Sample statistic
* A metric calculated for a sample of data drawn from a larger population.

Data distribution
* The frequency distribution of individual values in a data set.

Sampling distribution
* The frequency distribution of a sample statistic over many samples or resamples.

Central limit theorem
* The tendency of the sampling distribution to take on a normal shape as sample size rises.

Standard error
* The variability (standard deviation) of a sample statistic over many samples (not to be confused with standard deviation, which,
by itself, refers to variability of individual data values).

The histogram of the individual data values is broadly spread out and skewed toward higher values as is
to be expected with income data. The histograms of the means of 5 and 20 are increasingly compact and
more bell-shaped.

```{r histosamples, eval=FALSE, include=FALSE}

library(ggplot2)

loans_income <- read.csv(file="C:/Users/Emilio Blanco/Documents/Emilio/DS/datascience-scripts/psds_data/loans_income.csv")
# take a simple random sample
samp_data <- data.frame(income=sample(loans_income, 1000, TRUE), type='data_dist')
# take a sample of means of 5 values
samp_mean_05 <- data.frame(income = tapply(sample(loans_income, 1000*5, TRUE), rep(1:1000, rep(5, 1000)), FUN=mean), type = 'mean_of_5')
# take a sample of means of 20 values
samp_mean_20 <- data.frame(income = tapply(sample(loans_income, 1000*20, TRUE), rep(1:1000, rep(20, 1000)), FUN=mean), type = 'mean_of_20')
# bind the data.frames and convert type to a factor
income <- rbind(samp_data, samp_mean_05, samp_mean_20)
income$type = factor(income$type, levels=c('data_dist', 'mean_of_5', 'mean_of_20'), labels=c('Data', 'Mean of 5', 'Mean of 20'))
# plot the histograms
ggplot(income, aes(x=income)) + geom_histogram(bins=40) + facet_grid(type ~ .)

```



<!-- pagina 110 -->